{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sys\n",
    "\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "split_df=pd.read_csv('annotation_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tag\n",
    "def get_pos(tokens):\n",
    "    pos_tags_raw=nltk.pos_tag(tokens)\n",
    "    pos_tags=list(map(lambda x:x[1],pos_tags_raw))\n",
    "    return pos_tags\n",
    "\n",
    "# stop word\n",
    "def get_stop_word(tokens):\n",
    "    stop_words_list=stopwords.words('english')\n",
    "    stop_words=list(map(lambda x:x in stop_words_list,tokens))\n",
    "    return stop_words\n",
    "\n",
    "# NER\n",
    "# https://nlp.stanford.edu/software/CRF-NER.shtml\n",
    "def get_ner(tokens):\n",
    "    model_path='stanford_ner\\\\english.muc.7class.distsim.crf.ser.gz'\n",
    "    # model_path='stanford_ner\\\\english.all.3class.distsim.crf.ser.gz'\n",
    "    jar_path='stanford_ner\\\\stanford-ner.jar'\n",
    "    tagger = StanfordNERTagger(model_path,jar_path,encoding='utf-8')\n",
    "    # tagger = StanfordNERTagger(model_path,encoding='utf-8')\n",
    "    tagged = tagger.tag(tokens)\n",
    "    tagged=list(map(lambda x:x[1],tagged))\n",
    "    return tagged\n",
    "\n",
    "# upper/lower case information, acronyms, punctuation marks, etc.\n",
    "# % of upper case (or number)\n",
    "def get_syntactic(tokens):\n",
    "    return list(map(lambda x:sum(np.array(list(x.upper()))==np.array(list(x)))/len(x),tokens))\n",
    "\n",
    "# word2vec or bert representation\n",
    "def get_word2vec(tokens):\n",
    "    res=[]\n",
    "    for token in tokens:\n",
    "        try:wt=w2v_model.wv.get_vector(token)\n",
    "        except:wt=np.zeros(100)\n",
    "        res.append(wt)\n",
    "    return np.array(res)\n",
    "\n",
    "# bert representation\n",
    "def load_bert():\n",
    "    input_ids = tf.keras.layers.Input(shape=(3), dtype=tf.int32, name=\"input_ids\")\n",
    "\n",
    "    embedding = transformers.TFBertModel.from_pretrained(\"cambridgeltl/BioRedditBERT-uncased\")\n",
    "    out = embedding(input_ids)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_ids], outputs=out[0])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),loss=\"categorical_crossentropy\",metrics=[\"acc\"],)\n",
    "    return model\n",
    "\n",
    "def get_bert(tokens):\n",
    "    def tokenize_data(data):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"cambridgeltl/BioRedditBERT-uncased\")\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            data,\n",
    "            add_special_tokens=True,\n",
    "            max_length=3,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        return np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "\n",
    "    token_bert=tokenize_data(tokens)\n",
    "    emb=bert_model.predict(token_bert)\n",
    "    return emb.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens=split_df.Word.values\n",
    "\n",
    "split_df['pos']=get_pos(tokens)\n",
    "split_df['stop_word']=get_stop_word(tokens)\n",
    "# split_df['ner']=get_ner(tokens)\n",
    "split_df['synt']=get_syntactic(tokens)\n",
    "tokens_lower=list(map(lambda x:x.lower(),tokens))\n",
    "\n",
    "# embeddings\n",
    "corpus = api.load('text8') \n",
    "w2v_model = Word2Vec(corpus)  # time consuming\n",
    "temp=get_word2vec(tokens_lower)\n",
    "for i in range(temp.shape[1]):\n",
    "    split_df['word_2vec_%s'%(i+1)]=temp[:,i]\n",
    "\n",
    "bert_model=load_bert()\n",
    "temp=get_bert(tokens)\n",
    "for i in range(temp.shape[1]):\n",
    "    split_df['bert_%s'%(i+1)]=temp[:,i]\n",
    "\n",
    "split_df.to_csv('annotations.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37364bitbasecondaf759eddef18748939e267e7f3a31bd99",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "78c62bd594015071fc94ec09f8f0c8693d5df646d29fd961f83262230d581249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}